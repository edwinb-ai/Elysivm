var documenterSearchIndex = {"docs":
[{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"EditURL = \"<unknown>/src/examples/example2.jl\"","category":"page"},{"location":"example2/#Regression-on-a-synthetic-dataset","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"","category":"section"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"In this page we will see how to perform Least Squares Support Vector Regression using Elysivm. To accomplish this task, we will use synthetic data as created by the make_regression function from MLJ.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"First, we need to import all the necessary packages.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"import Elysivm\nusing MLJ, MLJBase\nusing DataFrames, CSV\nusing CategoricalArrays, Random\nusing Plots\ngr();\nrng = MersenneTwister(88);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We then create the regression problem. To really push the implementation we will create a problem with 5 features and 500 instances/observations. We will also add a little bit of Gaussian noise to the problem.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"X, y = MLJ.make_regression(500, 5; noise=1.0, rng=rng);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We need to construct a DataFrame with the arrays created to better handle the data, as well as a better integration with MLJ.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"df = DataFrame(X);\ndf.y = y;\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"A very important part of the MLJ framework is its use of scitypes, a special kind of types that work together with the objects from the framework. Because the regression problem has the Julia types we need to convert this types to correct scitypes such such that the machines from MLJ work fine.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"dfnew = coerce(df, autotype(df));\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can then observe the first three columns, together with their new types.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"first(dfnew, 3) |> pretty","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We should also check out the basic statistics of the dataset.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"describe(dfnew, :mean, :std, :eltype)","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Recall that we also need to standardize the dataset, we can see here that the mean is close to zero, but not quite, and we also need an unitary standard deviation.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Split the dataset into training and testing sets.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"y, X = unpack(dfnew, ==(:y), colname -> true);\ntrain, test = partition(eachindex(y), 0.75, shuffle=true, rng=rng);\nstand1 = Standardizer();\nX = MLJBase.transform(MLJBase.fit!(MLJBase.machine(stand1, X)), X);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We should make sure that the features have mean close to zero and an unitary standard deviation.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"describe(X |> DataFrame, :mean, :std, :eltype)","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Define a good set of hyperparameters for this problem and train the regressor. We will use the amazing capability of MLJ to tune a model and return the best model found.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"For this we have taken some judicious guessing on the best values that the hyperparameters should take. We employ 5-fold cross-validation and a 400 by 400 grid of points to do an exhaustive search.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We will train the regressor using the root mean square error which is defined as follows","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"RMSE = sqrtfracsum_i=1^N left(haty_i - y_i right)^2N","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"where we define haty_i as the predicted value, and y_i as the real value.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"model = Elysivm.LSSVRegressor();\nr1 = MLJBase.range(model, :σ, lower=7e-4, upper=1e-3);\nr2 = MLJBase.range(model, :γ, lower=120.0, upper=130.0);\nself_tuning_model = TunedModel(\n    model=model,\n    tuning=Grid(goal=400, rng=rng),\n    resampling=CV(nfolds=5),\n    range=[r1, r2],\n    measure=MLJBase.rms,\n    acceleration=CPUThreads(), # We use this to enable multithreading\n);\nnothing #hide","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"And now we proceed to train all the models and find the best one!","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"mach = MLJ.machine(self_tuning_model, X, y);\nMLJBase.fit!(mach, rows=train, verbosity=0);\nfitted_params(mach).best_model","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"Having found the best hyperparameters for the regressor model we proceed to check how the model generalizes and we use the test set to check the performance.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"ŷ = MLJBase.predict(mach, rows=test);\nresult = round(MLJBase.rms(ŷ, y[test]), sigdigits=4);\n@show result # Check the result","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can see that we did quite well. A value of 1, or close enough, is good. We expect it to reach a lower value, closer to zero, but maybe we needed more refinement in the grid search.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can also see a plot of the predicted and true values. The closer these dots are to the diagonal means that the model performed well.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"scatter(ŷ, y[test], markersize=9)\nr = range(minimum(y[test]), maximum(y[test]); length=length(test))\nplot!(r, r, linewidth=9)\nplot!(size=(3000, 2100))","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"We can actually see that we are not that far off, maybe a little more search could definitely improve the performance of our model.","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"","category":"page"},{"location":"example2/","page":"Regression on a synthetic dataset","title":"Regression on a synthetic dataset","text":"This page was generated using Literate.jl.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"EditURL = \"<unknown>/src/examples/example1.jl\"","category":"page"},{"location":"example1/#Classification-of-the-Wisconsin-breast-cancer-dataset","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"","category":"section"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"In this case study we will deal with the Wisconsin breast cancer dataset which can be browsed freely on the UCI website.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"In particular, this dataset contains 10 features and 699 instances. In the work we will do here, however, we will skip some instances due to some missing values.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"The dataset contains only two classes, and the purpose is to use all ten features to answer a simple question:","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Does the subject have a benign or malign tumor?","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"To answer this question, we will train a Least Squares Support Vector Machine as implemented in Elysivm.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"First, we need to import all the necessary packages.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"using MLJ, MLJBase\nusing DataFrames, CSV\nusing CategoricalArrays\nusing Random, Statistics\nimport Elysivm","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We then need to specify a seed to enable reproducibility of the results.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"rng = MersenneTwister(801239);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Here we are creating a list with all the headers.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"headers = [\n\t\"id\", \"Clump Thickness\",\n\t\"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\n\t\"Marginal Adhesion\", \"Single Epithelial Cell Size\",\n\t\"Bare Nuclei\", \"Bland Chromatin\",\n\t\"Normal Nucleoli\", \"Mitoses\", \"class\"\n];\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We define the path were the dataset is located","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"path = joinpath(\"examples\", \"wbc.csv\");\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We load the csv file and convert it to a DataFrame. Note that we are specifying to the file reader to replace the string ? to a missing value. This dataset contains the the string ? when there is a value missing.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"data = CSV.File(path; header=headers, missingstring=\"?\") |> DataFrame;\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We can display the first 10 rows from the dataset","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"first(data, 10)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We can see that all the features have been added correctly, we can see that we have an unncessary feature called id, so we will remove it.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"select!(data, Not(:id));\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We also need to remove all the missing data from the DataFrame","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"data = dropmissing(data);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"The class column should be of type categorical, following the MLJ API, so we encode it here.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"transform!(data, :class => categorical, renamecols=false);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Check statistics per column.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"describe(data)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Split the dataset into training and testing.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"y, X = unpack(data, ==(:class), colname -> true);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We will use only 2/3 for training.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"train, test = partition(eachindex(y), 2 / 3, shuffle=true, rng=rng);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Always remove mean and set the standard deviation to 1.0 when dealing with SVMs.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"stand1 = Standardizer(count=true);\nX = MLJBase.transform(fit!(machine(stand1, X)), X);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Check statistics per column again to ensure standardization, but remember to do it now with the X matrix.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"describe(X)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Good, now every column has a mean very close to zero, so the standardization was done correctly.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We now create our model with Elysivm","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"model = Elysivm.LSSVClassifier();\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"These are the values for the hyperparameter grid search. We need to find the best subset from this set of parameters. Although I will not do this here, the best approach is to find a set of good hyperparameters and then refine the search space around that set. That way we can ensure we will always get the best results.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"sigma_values = [0.5, 5.0, 10.0, 15.0, 25.0, 50.0, 100.0, 250.0, 500.0];\nr1 = MLJBase.range(model, :σ, values=sigma_values);\ngamma_values = [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0];\nr2 = MLJBase.range(model, :γ, values=gamma_values);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We now create a TunedModel that will use a 10-folds stratified cross validation scheme in order to find the best set of hyperparameters. The stratification is needed because the classes are somewhat imbalanced:","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Benign: 458 (65.5%)\nMalignant: 241 (34.5%)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"self_tuning_model = TunedModel(\n    model=model,\n    tuning=Grid(rng=rng),\n    resampling=StratifiedCV(nfolds=10),\n    range=[r1, r2],\n    measure=accuracy,\n    acceleration=CPUThreads(), # We use this to enable multithreading\n);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Once the best model is found, we create a machine with it, and fit it","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"mach = machine(self_tuning_model, X, y);\nfit!(mach, rows=train, verbosity=0);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"We can now show the best hyperparameters found.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"fitted_params(mach).best_model","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"And we test the trained model. We expect somewhere around 94%-96% accuracy.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"results = predict(mach, rows=test);\nacc = accuracy(results, y[test]);\nnothing #hide","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"Show the accuracy for the testing set","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"println(acc * 100.0)","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"As you can see, it is fairly easy to use Elysivm together with MLJ. We got a good accuracy result and this proves that the implementation is actually correct. This dataset is commonly used as a benchmark dataset to test new algorithms.","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"","category":"page"},{"location":"example1/","page":"Classification of the Wisconsin breast cancer dataset","title":"Classification of the Wisconsin breast cancer dataset","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Types","page":"Reference","title":"Types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Elysivm]\nPages = [\"types.jl\"]","category":"page"},{"location":"reference/#Elysivm.KernelRBF","page":"Reference","title":"Elysivm.KernelRBF","text":"KernelRBF\n\nThis type is to compute the RBF kernel defined as\n\nK(xy)=expleft( -vert x - yvert^2 gamma right)\n\nwhere vert x - yvert is the Euclidean norm. This norm is computed with the Kernels.jl package.\n\nFields\n\nγ::Real: The hyperparameter associated with the RBF kernel.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Elysivm.KernelRBF-Tuple{AbstractArray{T,2} where T,Float64}","page":"Reference","title":"Elysivm.KernelRBF","text":"KernelRBF(x::AbstractMatrix, gamma::Float64)\nKernelRBF(x, y::AbstractMatrix, gamma::Float64)\nKernelRBF(x, y::AbstractVector, gamma::Float64)\n\nDepending on the arguments, it computes either a pairwise RBF kernel (if it is only with one matrix), or a pairwise RBF kernel between a matrix and an array.\n\nArguments\n\nx::AbstractMatrix: A two-row matrix.\ny::AbstractVector: A one dimensional array.\ngamma::Float64: The hyperparameter needed to compute the kernel.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Elysivm.LSSVC","page":"Reference","title":"Elysivm.LSSVC","text":"LSSVC()\nLSSVC(; kernel=\"rbf\", γ=1.0, σ=1.0)\n\nThe type to hold a Least Squares Support Vector Classifier.\n\nFields\n\nkernel::String: The kind of kernel to use for the non-linear mapping of the data.\nγ::Float64: The gamma hyperparameter that is intrinsic of the Least Squares version of the Support Vector Machines.\nσ::Float64: The hyperparameter for the RBF kernel.\n\nKeywords\n\nkernel: A string to denote the kernel to be used.\nγ: A float value to assign the gamma hyperparameter.\nσ: A float value to assign the sigma hyperparameter.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Elysivm.LSSVR","page":"Reference","title":"Elysivm.LSSVR","text":"LSSVR()\nLSSVR(; kernel=\"rbf\", γ=1.0, σ=1.0)\n\nThe type to hold a Least Squares Support Vector Regressor.\n\nFields\n\nkernel::String: The kind of kernel to use for the non-linear mapping of the data.\nγ::Float64: The gamma hyperparameter that is intrinsic of the Least Squares version of the Support Vector Machines.\nσ::Float64: The hyperparameter for the RBF kernel.\n\nKeywords\n\nkernel: A string to denote the kernel to be used.\nγ: A float value to assign the gamma hyperparameter.\nσ: A float value to assign the sigma hyperparameter.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Elysivm.SVM","page":"Reference","title":"Elysivm.SVM","text":"SVM\n\nA super type for both classifiers and regressors that are implemented as Support Vector Machines.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Methods","page":"Reference","title":"Methods","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Elysivm]\nPages = [\"training.jl\"]","category":"page"},{"location":"reference/#Elysivm.build_omega-Tuple{AbstractArray{T,2} where T,AbstractArray{T,1} where T}","page":"Reference","title":"Elysivm.build_omega","text":"build_omega(x::AbstractMatrix, y::AbstractVector, sigma::Float64;\n    kernel::String=\"rbf\") -> AbstractMatrix\n\nIt builds a matrix, known as the \"omega matrix\", that contains the following information\n\nOmega_kl = y_k y_l K(x_k x_l)\n\nwith kl=1dotsN, and N being the length of x. In other words, the number of training instances.\n\nThis matrix contains information about the mapping to a new space using the kernel. It is exclusively used in the training step of the learning procedure.\n\nArguments\n\nx::AbstractMatrix: The data matrix with the training instances.\ny::AbstractVector: The labels for each of the instances in x.\n\nKeywords\n\nkernel::String=\"rbf\": The kernel to be used. For now, only the RBF kernel is implemented.\nsigma::Float64: The hyperparameter for the RBF kernel.\n\nReturns\n\nΩ: The omega matrix computed as shown above.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Elysivm.svmpredict-Tuple{LSSVC,Any,AbstractArray{T,2} where T}","page":"Reference","title":"Elysivm.svmpredict","text":"svmpredict(svm::LSSVC, fits, xnew::AbstractMatrix) -> AbstractArray\n\nUses the information obtained from svmtrain such as the bias and weights to construct a decision function and predict new class values.\n\nArguments\n\nsvm::LSSVC: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nfits: It can be any container data structure but it must have four elements: x, the data matrix; y, the labels vector; α, the weights; and b, the bias.\nxnew::AbstractMatrix: The data matrix that contains the new instances to be predicted.\n\nReturns\n\nArray: The labels corresponding to the prediction to each of the instances in xnew.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Elysivm.svmpredict-Tuple{LSSVR,Any,AbstractArray{T,2} where T}","page":"Reference","title":"Elysivm.svmpredict","text":"svmpredict(svm::LSSVR, fits, xnew::AbstractMatrix) -> AbstractArray\n\nUses the information obtained from svmtrain such as the bias and weights to construct a decision function and predict the new values of the function.\n\nArguments\n\nsvm::LSSVR: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nfits: It can be any container data structure but it must have four elements: x, the data matrix; y, the labels vector; α, the weights; and b, the bias.\nxnew::AbstractMatrix: The data matrix that contains the new instances to be predicted.\n\nReturns\n\nArray: The labels corresponding to the prediction to each of the instances in xnew.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Elysivm.svmtrain-Tuple{LSSVC,AbstractArray{T,2} where T,AbstractArray{T,1} where T}","page":"Reference","title":"Elysivm.svmtrain","text":"svmtrain(svm::LSSVC, x::AbstractMatrix, y::AbstractVector) -> Tuple\n\nSolves a Least Squares Support Vector Classification problem using the Conjugate Gradient method. In particular, it uses the Lanczos process due to the fact that the matrices are symmetric.\n\nArguments\n\nsvm::LSSVC: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nx::AbstractMatrix: The data matrix with the features. It is expected that this array is already standardized, i.e. the mean for each feature is zero and its standard deviation is one.\ny::AbstractVector: A vector that contains the classes. It is expected that there are only two classes, -1 and 1.\n\nReturns\n\nTuple: A tuple containing x, y and the following two elements:\nb: Contains the bias for the decision function.\nα: Contains the weights for the decision function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Elysivm.svmtrain-Tuple{LSSVR,AbstractArray{T,2} where T,AbstractArray{T,1} where T}","page":"Reference","title":"Elysivm.svmtrain","text":"svmtrain(svm::LSSVR, x::AbstractMatrix, y::AbstractVector) -> Tuple\n\nSolves a Least Squares Support Vector Regression problem using the Conjugate Gradient method. In particular, it uses the Lanczos process due to the fact that the matrices are symmetric.\n\nArguments\n\nsvm::LSSVR: The Support Vector Machine that contains the hyperparameters, as well as the kernel to be used.\nx::AbstractMatrix: The data matrix with the features. It is expected that this array is already standardized, i.e. the mean for each feature is zero and its standard deviation is one.\ny::AbstractVector: A vector that contains the classes. It is expected that there are only two classes, -1 and 1.\n\nReturns\n\nTuple: A tuple containing x and the following two elements:\nb: Contains the bias for the decision function.\nα: Contains the weights for the decision function.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Elysivm","category":"page"},{"location":"#Elysivm","page":"Home","title":"Elysivm","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is Elysivm, a Least Squares Support Vector Machine (LSSVM) implementation in pure Julia. It is meant to be used together with the fantastic MLJ.jl Machine Learning framework.","category":"page"},{"location":"#Formulation","page":"Home","title":"Formulation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"It is a re-formulation of the classical Support Vector Machine (SVM) formalism. In this case we attempt to solve a least squares problem which is faster[1], instead of the classic quadratic, convex optimization problem that is solved in the original Support Vector Machine.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the case of Elysivm we use the conjugate gradient method, in particular the Lanczos version[2] due to the fact that we solve several linear systems which have the the following structure","category":"page"},{"location":"","page":"Home","title":"Home","text":"A mathbfx = mathbfb","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the matrix A is symmetric.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This fact makes it a great candidate for the Lanczos algorithm, a very fast, iterative procedure based on Krylov subspace methods. The implementation used here is that from the Krylov.jl package.","category":"page"},{"location":"#Rationale","page":"Home","title":"Rationale","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SVM has been the most known and used formulation, but here are some pros and cons for using LSSVMs and Elysivm.","category":"page"},{"location":"#Advantages","page":"Home","title":"Advantages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The LSSVM is a great alternative to the classic SVM in the following things:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Solving a linear system is much easier and faster than solving a quadratic optimization problem.\nSome useful properties from numerical linear algebra can be exploited in order to solve the new optimization problem.\nOne can potentialy train of thousands or millions of instances using LSSVM, something that the classic SVM cannot do. This is possible using the fixed size LSSVM[3].\nLess hyperparemeters to tune. LSSVM only has one intrinsic hyperparamter, whereas the SVM has at least two. This is without taking into account the kernel's hyperparameter.","category":"page"},{"location":"#Disadvantages","page":"Home","title":"Disadvantages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"But there are some important shortcommings for the LSSVM, namely:","category":"page"},{"location":"","page":"Home","title":"Home","text":"In contrast with the classic SVM, the decision function lack all sparseness. Every single dataset instance must be used to train the LSSVM. This can become troublesome for very large problems because all the instances must fit into memory.\nThere is complete lack of interpretation for the support vectors, which are the data instances that are used to construct the decision function. Because all data instances are used, every instance is effectively a support vector and this removes any interpretation for the importance of each instance on the model's performance.","category":"page"},{"location":"#Bibliography","page":"Home","title":"Bibliography","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[1]: Suykens, J. A., & Vandewalle, J. (1999). Least squares support vector machine classifiers. Neural processing letters, 9(3), 293-300.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[2]: Fasano, G. (2007). Lanczos conjugate-gradient method and pseudoinverse computation on indefinite and singular systems. Journal of optimization theory and applications, 132(2), 267-285.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[3]: Espinoza, M., Suykens, J. A., & De Moor, B. (2006). Fixed-size least squares support vector machines: A large scale application in electrical load forecasting. Computational Management Science, 3(2), 113-129.","category":"page"}]
}
